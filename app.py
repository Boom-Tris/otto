import streamlit as st
import pandas as pd
import joblib
import lightgbm as lgb
import gdown
import os

from collections import Counter

# --- ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà (‡∏Ñ‡∏ß‡∏£‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Cell 2 ‡πÉ‡∏ô Notebook) ---
# (‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡πâ‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏™‡πà‡∏á‡∏°‡∏≤)
N_ITEMS_FROM_HISTORY = 5
N_CO_VISITS_PER_ITEM = 40
N_CANDIDATES_PER_SESSION = 200

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) ---
def load_model(path):
    """
    ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• LightGBM ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    - ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô sklearn wrapper (LGBMRanker) ‡πÉ‡∏ä‡πâ joblib.load()
    - ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô native Booster ‡πÉ‡∏ä‡πâ lgb.Booster(model_file)
    """
    try:
        # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏õ‡πá‡∏ô sklearn wrapper ‡∏Å‡πà‡∏≠‡∏ô
        model = joblib.load(path)
        if hasattr(model, "predict"):
            print(f"‚úÖ Loaded sklearn model: {path}")
            return model
        else:
            raise TypeError("Not sklearn model")
    except Exception:
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à, ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏õ‡πá‡∏ô native Booster
        print(f"‚öôÔ∏è Loading as Booster model: {path}")
        return lgb.Booster(model_file=path)

# --- 1. ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á (Models + Maps) ---
@st.cache_resource
def load_all_assets():
    # ‡πÇ‡∏´‡∏•‡∏î co_visitation_map ‡∏à‡∏≤‡∏Å google drive
    print("--- 0. Downloading co_visitation_map from Google Drive")
    
    if not os.path.exists("co_visitation_map.joblib"):
        gdown.download("https://drive.google.com/uc?id=1PMs1-swsSPwyH-_nL2IqX5Bz5p34mb9N", "co_visitation_map.joblib", quiet=False)
    
    if not os.path.exists("top_20_fallback.joblib"):
        gdown.download("https://drive.google.com/uc?id=14FwUZeiXM9ZRfyS45EMzOuAJtQkPRjSt", "top_20_fallback.joblib", quiet=False)

    if not os.path.exists("global_popularity_counter.joblib"):
        gdown.download("https://drive.google.com/uc?id=1Q5OdFMmGh34fw-ZYfcC3GwQ4d60932YQ", "global_popularity_counter.joblib", quiet=False)
    
    if not os.path.exists("lgbm_ranker_clicks.pkl"):
        gdown.download("https://drive.google.com/uc?id=1oniZl-45sxldTYNeB8ruY_4VY2_TD02g", "lgbm_ranker_clicks.pkl", quiet=False)
    
    if not os.path.exists("lgbm_ranker_carts.pkl"):
        gdown.download("https://drive.google.com/uc?id=1ZFD7d9ehJ-T5C4aErJ08UCiyP5mKbcri", "lgbm_ranker_carts.pkl", quiet=False)
    
    if not os.path.exists("lgbm_ranker_orders.pkl"):
        gdown.download("https://drive.google.com/uc?id=1oWaLZi6Jzrc2YmSKqF3SnHorFGACcVQQ", "lgbm_ranker_orders.pkl", quiet=False)

    # ‡πÇ‡∏´‡∏•‡∏î Model ‡πÅ‡∏•‡∏∞ Maps ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡πÉ‡∏ä‡πâ cache ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß)
    print("--- 1. Loading all assets... ---")
    try:
        models = {
            "clicks": load_model("lgbm_ranker_clicks.pkl"),
            "carts": load_model("lgbm_ranker_carts.pkl"),
            "orders": load_model("lgbm_ranker_orders.pkl")
        }
        
        # (‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ .joblib ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà Notebook ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å)
        global_popularity_counter = joblib.load("global_popularity_counter.joblib")
        co_visitation_map = joblib.load("co_visitation_map.joblib")
        top_20_fallback = joblib.load("top_20_fallback.joblib")

        # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠ Feature 4 ‡∏ï‡∏±‡∏ß‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
        # (‡πÄ‡∏£‡∏≤‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ model 'clicks' ‡πÄ‡∏õ‡πá‡∏ô sklearn wrapper ‡∏ó‡∏µ‡πà‡∏°‡∏µ .feature_name_)
        # ‡∏ñ‡πâ‡∏≤ Error ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô ['co_visitation_score', 'global_popularity', 'session_length', 'history_clicks_on_candidate']
        if hasattr(models['clicks'], 'feature_name_'):
             feature_names = models['clicks'].feature_name_
        else:
             # Fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö native booster
             feature_names = models['clicks'].feature_name()

        print(f"Models loaded. Expecting features: {feature_names}")
        
        return models, global_popularity_counter, co_visitation_map, top_20_fallback, feature_names

    except FileNotFoundError as e:
        st.error(f"‚ùå ERROR: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏£‡∏∑‡∏≠ Map! - {e}")
        st.error("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏±‡∏ô NewV_2.ipynb (Cell 3, 5, 6) [cite: NewV_2.ipynb, Cell 3, 5, 6] ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå .joblib ‡πÅ‡∏•‡∏∞ .pkl ‡∏°‡∏≤‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡∏Å‡πà‡∏≠‡∏ô")
        st.stop()
    except Exception as e:
        st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå: {e}")
        st.stop()

# --- ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---
models, global_popularity_counter, co_visitation_map, top_20_fallback, FEATURE_NAMES = load_all_assets()


# --- 2. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô Cell 8) [cite: NewV_2.ipynb, Cell 8] ---
def get_top_20_recs(scores, candidates, fallback):
    """
    ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô, ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Top 20, ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢ fallback ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏≠
    """
    ranked_candidates = [aid for _, aid in sorted(zip(scores, candidates), reverse=True)]
    top_20 = ranked_candidates[:20]
    if len(top_20) < 20:
        top_20.extend([aid for aid in fallback if aid not in top_20])
        top_20 = top_20[:20]
    return top_20


# --- 3. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏ï‡∏≤‡∏° Cell 8) [cite: NewV_2.ipynb, Cell 8] ---
def run_model_pipeline(session_data):
    """
    ‡∏£‡∏±‡∏ô 2-Stage Pipeline (Candidate Generation -> Ranking)
    """
    events = session_data["events"]
    
    if not events:
        # ‡∏ñ‡πâ‡∏≤ session ‡∏ß‡πà‡∏≤‡∏á, ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ fallback
        fallback_str = " ".join(map(str, top_20_fallback))
        return {
            "clicks": fallback_str,
            "carts": fallback_str,
            "orders": fallback_str
        }, pd.DataFrame(columns=FEATURE_NAMES) # ‡∏Ñ‡∏∑‡∏ô DF ‡∏ß‡πà‡∏≤‡∏á

    # --- 3.1. Stage 1: Candidate Generation (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô Cell 8) [cite: NewV_2.ipynb, Cell 8] ---
    session_candidate_pool = Counter()
    history_aids = [event['aid'] for event in events]
    history_aids_set = set(history_aids)
    history_aids_recent = history_aids[-N_ITEMS_FROM_HISTORY:]

    for aid in set(history_aids_recent):
        top_co_visited = co_visitation_map.get(aid, Counter()).most_common(N_CO_VISITS_PER_ITEM)
        for co_aid, score in top_co_visited:
            if co_aid not in history_aids_set: # ‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Ç‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡πâ‡∏ß
                session_candidate_pool[co_aid] += score
                
    dynamic_candidates = [aid for aid, score in session_candidate_pool.most_common(N_CANDIDATES_PER_SESSION)]
    # ‡∏£‡∏ß‡∏° "‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å" ‡∏à‡∏≤‡∏Å co-visit ‡πÅ‡∏•‡∏∞ "‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å" ‡∏¢‡∏≠‡∏î‡∏Æ‡∏¥‡∏ï (fallback)
    final_candidate_list = list(dict.fromkeys(dynamic_candidates + top_20_fallback))
    final_candidate_list = final_candidate_list[:N_CANDIDATES_PER_SESSION]

    if not final_candidate_list:
        # (‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà 2: ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤ candidate ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢)
        fallback_str = " ".join(map(str, top_20_fallback))
        return {
            "clicks": fallback_str,
            "carts": fallback_str,
            "orders": fallback_str
        }, pd.DataFrame(columns=FEATURE_NAMES)

    # --- 3.2. Stage 2: Ranking (‡∏™‡∏£‡πâ‡∏≤‡∏á 4 Features ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô Cell 8) [cite: NewV_2.ipynb, Cell 8] ---
    session_length = len(events)
    history_aids_counter = Counter(history_aids)
    X_test_session_list = []
    
    for candidate_aid in final_candidate_list:
        
        # Feature 1: Co-visitation Score [cite: NewV_2.ipynb, Cell 4, 8]
        co_visit_score = 0
        for history_aid in history_aids_set:
            co_visit_score += co_visitation_map.get(history_aid, Counter()).get(candidate_aid, 0)

        # Feature 2: Global Popularity [cite: NewV_2.ipynb, Cell 4, 8]
        global_pop = global_popularity_counter.get(candidate_aid, 0)
        
        # Feature 3: Session Length [cite: NewV_2.ipynb, Cell 4, 8]
        session_len = session_length
        
        # Feature 4: History Clicks (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡πÉ‡∏ô‡∏≠‡∏î‡∏µ‡∏ï) [cite: NewV_2.ipynb, Cell 4, 8]
        history_clicks = history_aids_counter.get(candidate_aid, 0)

        features = [
            co_visit_score,
            global_pop,
            session_len,
            history_clicks
        ]
        X_test_session_list.append(features)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame Input
    X_df = pd.DataFrame(X_test_session_list, columns=FEATURE_NAMES, index=final_candidate_list)
    
    # --- 3.3. ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö ---
    try:
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡πà‡∏≠‡∏ô predict
        if isinstance(models['clicks'], lgb.Booster):
            # Native booster
            scores_clicks = models['clicks'].predict(X_df.values, num_iteration=models['clicks'].best_iteration)
            scores_carts = models['carts'].predict(X_df.values, num_iteration=models['carts'].best_iteration)
            scores_orders = models['orders'].predict(X_df.values, num_iteration=models['orders'].best_iteration)
        else:
            # Sklearn wrapper
            scores_clicks = models['clicks'].predict(X_df)
            scores_carts = models['carts'].predict(X_df)
            scores_orders = models['orders'].predict(X_df)
    except Exception as e:
        st.error(f"Error during prediction: {e}")
        return {}, pd.DataFrame()


    # ‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö
    top_20_clicks = get_top_20_recs(scores_clicks, final_candidate_list, top_20_fallback)
    top_20_carts = get_top_20_recs(scores_carts, final_candidate_list, top_20_fallback)
    top_20_orders = get_top_20_recs(scores_orders, final_candidate_list, top_20_fallback)

    results = {
        "clicks": " ".join(map(str, top_20_clicks)),
        "carts": " ".join(map(str, top_20_carts)),
        "orders": " ".join(map(str, top_20_orders))
    }
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô DF ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏ä‡∏ß‡πå (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Debug)
    X_df['score_clicks'] = scores_clicks
    X_df['score_carts'] = scores_carts
    X_df['score_orders'] = scores_orders
    
    # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ 2 ‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (dict) ‡πÅ‡∏•‡∏∞ DF (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏ä‡∏ß‡πå)
    return results, X_df.sort_values('score_orders', ascending=False) # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° Order score


# --- 4. ‡∏™‡πà‡∏ß‡∏ô UI ‡∏Ç‡∏≠‡∏á Streamlit (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡∏°‡πà) ---
if __name__ == "__main__":
    try:
        # (‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏Ñ‡πà 100 ‡πÅ‡∏ñ‡∏ß‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á)
        samples = pd.read_json(path_or_buf="test_trimmed.jsonl", lines=True, nrows=100)
    except FileNotFoundError:
        st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå test_trimmed.jsonl")
        st.info("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå test_trimmed.jsonl ‡∏à‡∏≤‡∏Å Kaggle ‡∏°‡∏≤‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö app.py")
        st.stop()
    except Exception as e:
        st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô test_trimmed.jsonl: {e}")
        st.stop()


    st.title("üß† OTTO: Recommender System (v2 - 4 Features)")

    with st.container():
        sample_index = st.selectbox("Choose Sample Session No.", samples.index)
        selected_sample = samples.iloc[sample_index]

    with st.container(border=True, height=320):
        st.write(f"**Session ID:** `{selected_sample['session']}`")
        st.write("**Events (History):**")
        st.json(selected_sample["events"]) # ‡πÉ‡∏ä‡πâ st.json ‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô

    with st.container():
        if st.button("üöÄ Run Prediction Pipeline", type="primary", use_container_width=True):
            st.divider()
            
            # --- ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà ---
            with st.spinner("Finding Candidates and Ranking..."):
                results, features_df = run_model_pipeline(selected_sample)
            
            st.subheader("üîÆ Prediction Results (Top 20)")
            st.info("‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ Top 20 'aid' ‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï")
            st.json(results) # ‡πÉ‡∏ä‡πâ st.json ‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
            
            st.divider()
            st.subheader(f"üìä Features & Scores for {len(features_df)} Candidates")
            st.info("‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ Input 4-Features (‡πÅ‡∏•‡∏∞ Scores) ‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ä‡πâ (‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° Order Score)")
            st.dataframe(features_df.head(20))
